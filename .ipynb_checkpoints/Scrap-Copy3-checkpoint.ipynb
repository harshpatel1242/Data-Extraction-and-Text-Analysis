{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86172427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "# nltk.download('punkt')\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb708d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xlsx....csv.....float to int\n",
    "inp = pd.DataFrame(pd.read_excel(\"D:/Home_Extras/Black_Coffer/InternshipTasks/Assignment/Input.xlsx\"))\n",
    "inp['URL_ID'] = inp['URL_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "234f1545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging all stop words into one txt file\n",
    "file_names = ['StopWords_Auditor.txt', 'StopWords_Currencies.txt', 'StopWords_DatesandNumbers.txt', 'StopWords_Generic.txt', 'StopWords_GenericLong.txt', 'StopWords_Geographic.txt', 'StopWords_Names.txt']\n",
    "with open(\"stw.txt\", \"w\") as outfile:\n",
    "    for f in file_names:\n",
    "        with open(f) as infile:\n",
    "            outfile.write(infile.read())\n",
    "        outfile.write(\"\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "779f370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopList = open(\"stw.txt\").readlines()\n",
    "stop_words=[]\n",
    "for element in stopList:\n",
    "    stop_words.append(element.strip())\n",
    "#stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b03f38ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inp = inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ab4a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inp['text'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f63408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f9026c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp/ipykernel_19308/3282793742.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_inp['text'][i] = f.read()\n"
     ]
    }
   ],
   "source": [
    "#function creates txt file\n",
    "def createcsv(link, idd_conv,i):\n",
    "    page = requests.get(link,headers=headers)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    Title= soup.find(\"title\").getText()\n",
    "    Content= soup.find(\"div\", class_=\"td-post-content\").getText()\n",
    "    #create new file add title\n",
    "    f = open(idd_conv,'w', encoding=\"utf-8\")\n",
    "    f.write(Title)\n",
    "    f.close()\n",
    "    #append content to the created file    \n",
    "    with open(idd_conv,'a', encoding=\"utf-8\") as f:\n",
    "        f.write(Content)\n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "#function will add text to new column \"text\"   \n",
    "def add_nc(idd_conv,i):    \n",
    "    with open(idd_conv,'r', encoding=\"utf-8\") as f:    \n",
    "        new_inp['text'][i] = f.read()\n",
    "        f.close()\n",
    "    #check partivular original text from website using print    \n",
    "#     f = open(\"1.txt\", \"r\", encoding=\"utf-8\")\n",
    "#     print(f.read())\n",
    "\n",
    "for i in range(len(inp[\"URL_ID\"])):\n",
    "    link = inp[\"URL\"][i]\n",
    "    idd = inp[\"URL_ID\"][i]\n",
    "    #variable added to string\n",
    "    idd_conv = '%s.txt' % idd\n",
    "    #print(idd_conv)\n",
    "    #commenting cuz aleardy done  \n",
    "    #createcsv(link, idd_conv,i)   \n",
    "    add_nc(idd_conv,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07d8567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "478d27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inp[\"cleaned_text\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f10a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function does preprocessing task \n",
    "def clean_it(idd,i):\n",
    "    #tokenization\n",
    "    text = idd\n",
    "    #normalization\n",
    "#     text = [word for word in text if word.isalpha()]\n",
    "#     #stemming and lemmitizing together\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     ps = PorterStemmer()\n",
    "#     text = [lemmatizer.lemmatize(word) if word.endswith('e') else ps.stem(word) for word in text]\n",
    "#     #removing stop words\n",
    "#     cleaned_words = [word for word in text if word.lower() not in stop_words]\n",
    "#     #list to string\n",
    "#     cleaned_txt = \" \".join(cleaned_words)\n",
    "#     new_inp[\"cleaned_text\"][i] = cleaned_txt\n",
    "#run through each file  \n",
    "for i in range(len(new_inp[\"text\"][:1])): \n",
    "    idd = new_inp[\"text\"][i]\n",
    "    clean_it(idd,i)\n",
    "    #cleaned  for one file have to run for loop tto clean all files and saving file remaninng    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ca7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9255a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #cleaning\n",
    "# def clean_text(each_file):\n",
    "#     cleaned = [word for word in each_file.split(\" \") if word not in stoplist]\n",
    "#     #list_to_text\n",
    "#     cleaned_file= ' '.join(word for word in cleaned)\n",
    "#     return cleaned_file\n",
    "    \n",
    "# #funnction edits the original file with clean text\n",
    "# def create_cleaned(write_file,open_file):\n",
    "#     #edit the original file\n",
    "#     f = open(open_file,'w', encoding=\"utf-8\")\n",
    "#     f.write(write_file)\n",
    "#     f.close()\n",
    "# #run through each file    \n",
    "# for i in range(len(inp[\"URL_ID\"][:1])):\n",
    "#     idd = inp[\"URL_ID\"][i]\n",
    "#     #variable added to string\n",
    "#     idd_conv = '%s.txt' % idd\n",
    "#     org_file = open(idd_conv, encoding=\"utf-8\").read()\n",
    "#     clean_text(org_file)\n",
    "#     create_cleaned(cleaned_file, org_file)\n",
    "#     #cleaned  for one file have to run for loop tto clean all files and saving file remaninng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a03f73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inp[\"tokens_text\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "653311dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp/ipykernel_19308/2041606934.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_inp[\"tokens_text\"][k] = tokens\n"
     ]
    }
   ],
   "source": [
    "def create_tokens(k):\n",
    "    prp_text = new_inp['cleaned_text'][k]\n",
    "    tokens = regexp_tokenize(prp_text, \"[\\w']+\")\n",
    "    new_inp[\"tokens_text\"][k] = tokens\n",
    "for k in range(len(new_inp[\"cleaned_text\"])):\n",
    "    create_tokens(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a96f021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_inp[\"tokens_text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7734b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_org = pd.read_csv('./PN/Pos.txt', names=['pos'])\n",
    "# pos_org_cleaned = [word for word in pos_org if word.lower() in stop_words]\n",
    "pos_words= [] \n",
    "# pos_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af2bfe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in pos_org['pos']:\n",
    "    pos_words.append(element.strip())\n",
    "# pos_words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b56ef4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_org = pd.read_csv('./PN/Neg.txt', names=['neg'])\n",
    "neg_words= [] \n",
    "# neg_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7ff12a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in neg_org['neg']:\n",
    "    neg_words.append(element.strip())\n",
    "# neg_words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6d47b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_score = 0\n",
    "neg_score = 0\n",
    "for word in new_inp[\"tokens_text\"][0]:\n",
    "    if word in pos_words:\n",
    "        pos_score +=1\n",
    "    elif word in neg_words:\n",
    "        neg_score -=1\n",
    "neg_score = neg_score*-1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7debe96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d58c406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80fc2e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pola_score = (pos_score-neg_score)/ ((pos_score + neg_score) + 0.000001)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "044a737f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pola_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5365c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_score = (pos_score + neg_score)/ (len(new_inp[\"tokens_text\"][0]) + 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25d3e53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e207ab7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 5, 6, 4, 8, 3, 9, 2, 6, 4, 2, 2, 11, 8, 4, 6, 4, 2, 4, 5, 5, 5, 8, 6, 3, 5, 6, 2, 5, 2, 4, 4, 7, 3, 6, 3, 4, 5, 1, 4, 2, 1, 3, 9, 7, 13, 2, 4, 4, 7, 8, 8, 3, 2, 7, 8, 2, 3, 8, 10, 2, 4, 5, 3, 6, 2, 7, 6, 2, 11, 4, 3, 8, 7, 10, 2, 7, 11, 3, 2, 4, 4, 7, 4, 3, 6, 15, 2, 4, 4, 4, 7, 3, 5, 9, 2, 2, 8, 3, 9, 4, 5, 2, 1, 8, 13, 3, 11, 2, 4, 3, 10, 3, 7, 4, 3, 1, 5, 5, 10, 12, 2, 2, 1, 7, 3, 1, 3, 2, 13, 12, 4, 3, 8, 2, 3, 9, 7, 5, 11, 4, 3, 4, 6, 3, 5, 7, 7, 3, 6, 2, 5, 5, 6, 3, 4, 7, 1, 6, 5, 3, 4, 1, 3, 4, 2, 3, 3, 2, 3, 3, 2, 8, 7, 2, 3, 5, 8, 3, 5, 5, 5, 12, 10, 5, 2, 11, 8, 7, 10, 10, 3, 6, 7, 10, 3, 8, 4, 6, 4, 2, 3, 3, 5, 7, 4, 9, 4, 9, 4, 3, 2, 8, 2, 5, 8, 7, 3, 4, 9, 3, 10, 2, 6, 6, 2, 3, 4, 8, 10, 8, 2, 6, 5, 7, 4, 5, 6, 4, 8, 2, 4, 9, 2, 4, 6, 5, 2, 5, 4, 6, 4, 2, 5, 10, 10, 5, 4, 2, 4, 9, 2, 1, 4, 8, 5, 5, 4, 8, 3, 1, 2, 9, 3, 2, 3, 6, 1, 6, 2, 10, 4, 10, 4, 8, 4, 6, 1, 5, 8, 3, 2, 4, 4, 3, 7, 2, 11, 4, 2, 8, 10, 5, 4, 3, 8, 2, 5, 4, 2, 8, 2, 8, 2, 7, 6, 2, 5, 4, 8, 2, 3, 4, 6, 3, 8, 5, 4, 3, 6, 11, 4, 3, 7, 5, 5, 6, 6, 2, 9, 5, 2, 2, 6, 2, 3, 4, 8, 4, 8, 1, 5, 4, 5, 7, 5, 3, 7, 5, 2, 8, 3, 8, 2, 2, 8, 2, 3, 6, 2, 4, 6, 3, 7, 3, 6, 2, 3, 3, 6, 3, 4, 8, 5, 2, 5, 7, 6, 3, 6, 3, 4, 4, 4, 4, 2, 5, 9, 2, 9, 3, 4, 6, 2, 7, 2, 11, 1, 6, 4, 2, 3, 2, 5, 6, 2, 2, 4, 8, 9, 2, 6, 5, 4, 5, 4, 4, 6, 9, 6, 3, 6, 3, 9, 3, 4, 2, 3, 8, 1, 10, 10, 5, 3, 7, 9, 11, 6, 4, 3, 8, 4, 4, 9, 9, 4, 3, 11, 3, 7, 5, 9, 2, 4, 9, 2, 3, 5, 2, 7, 4, 3, 5, 12, 2, 1, 7, 4, 2, 8, 3, 8, 4, 2, 4, 2, 7, 3, 7, 3, 7, 5, 3, 3, 8, 4, 3, 2, 4, 4, 2, 3, 6, 2, 8, 8, 9, 1, 11, 8, 5, 4, 2, 5, 10, 4, 10, 5, 2, 6, 10, 6, 6, 2, 3, 5, 5, 11, 11, 3, 4, 2, 3, 2, 2, 1, 4, 2, 1, 8, 1, 12, 2, 4, 6, 5, 5, 3, 2, 4, 4, 2, 6, 2, 8, 9, 1, 8, 5, 6, 8, 2, 6, 3, 5, 2, 9, 10, 8, 10, 12, 3, 10, 6, 5, 3, 6, 7, 3, 6, 8, 3, 11, 9, 2, 5, 10, 6, 5, 7, 5, 3, 2, 6, 6, 5, 10, 5, 3, 7, 8, 8, 4, 8, 2, 11, 1, 2, 3, 4, 2, 5, 6, 8, 4, 4, 4, 4, 9, 7, 7, 7, 4, 6, 2, 4, 5, 10, 7, 3, 6, 4, 5, 3, 2, 7, 5, 4, 8, 1, 6, 2, 3, 3, 4, 2, 4, 4, 3, 11, 4, 7, 1, 4, 3, 3, 5, 2, 7, 5, 8, 5, 2, 4, 5, 2, 3, 7, 3, 7, 3, 6, 2, 2, 3, 2, 9, 3, 2, 5, 7, 3, 6, 2, 4, 6, 4, 6, 5, 3, 5, 8, 5, 4, 3, 3, 2, 2, 3, 4, 8, 12, 2, 3, 8, 4, 8, 5, 4, 4, 13, 5, 6, 4, 4, 9, 2, 4, 5, 7, 8, 4, 5, 2, 6, 4, 5, 5, 6, 9, 3, 5, 2, 6, 5, 8, 4, 2, 3, 11, 6, 11, 8, 2, 6, 1, 7, 9, 2, 10]\n",
      "['How', 'is', 'Login', 'Logout', 'Time', 'Tracking', 'for', 'Employees', 'in', 'Office', 'done', 'by', 'AI', 'Blackcoffer', 'Insights', 'When', 'people', 'hear', 'AI', 'they', 'often', 'think', 'about', 'sentient', 'robots', 'and', 'magic', 'boxes.', 'AI', 'today', 'is', 'much', 'more', 'mundane', 'and', 'simple', 'but', 'that', 'doesn', 't', 'mean', 'it', 's', 'not', 'powerful.', 'Another', 'misconception', 'is', 'that', 'high', 'profile', 'research', 'projects', 'can', 'be', 'applied', 'directly', 'to', 'any', 'business', 'situation.', 'AI', 'done', 'right', 'can', 'create', 'an', 'extreme', 'return', 'on', 'investments', 'ROIs', 'for', 'instance', 'through', 'automation', 'or', 'precise', 'prediction.', 'But', 'it', 'does', 'take', 'thought', 'time', 'and', 'proper', 'implementation.', 'We', 'have', 'seen', 'that', 'success', 'and', 'value', 'generated', 'by', 'AI', 'projects', 'are', 'increased', 'when', 'there', 'is', 'a', 'grounded', 'understanding', 'and', 'expectation', 'of', 'what', 'the', 'technology', 'can', 'deliver', 'from', 'the', 'C', 'suite', 'down.', 'Artificial', 'Intelligence', 'AI', 'is', 'a', 'science', 'and', 'a', 'set', 'of', 'computational', 'technologies', 'that', 'are', 'inspired', 'by', 'but', 'typically', 'operate', 'quite', 'differently', 'from', 'the', 'ways', 'people', 'use', 'their', 'nervous', 'systems', 'and', 'bodies', 'to', 'sense', 'learn', 'reason', 'and', 'take', 'action.', '3', 'Lately', 'there', 'has', 'been', 'a', 'big', 'rise', 'in', 'the', 'day', 'to', 'day', 'use', 'of', 'machines', 'powered', 'by', 'AI.', 'These', 'machines', 'are', 'wired', 'using', 'cross', 'disciplinary', 'approaches', 'based', 'on', 'mathematics', 'computer', 'science', 'statistics', 'psychology', 'and', 'more.4', 'Virtual', 'assistants', 'are', 'becoming', 'more', 'common', 'most', 'of', 'the', 'web', 'shops', 'predict', 'your', 'purchases', 'many', 'companies', 'make', 'use', 'of', 'chatbots', 'in', 'their', 'customer', 'service', 'and', 'many', 'companies', 'use', 'algorithms', 'to', 'detect', 'fraud.', 'AI', 'and', 'Deep', 'Learning', 'technology', 'employed', 'in', 'office', 'entry', 'systems', 'will', 'bring', 'proper', 'time', 'tracking', 'of', 'each', 'employee.', 'As', 'this', 'system', 'tries', 'to', 'learn', 'each', 'person', 'with', 'an', 'image', 'processing', 'technology', 'whose', 'data', 'is', 'feed', 'forwarded', 'to', 'a', 'deep', 'learning', 'model', 'where', 'Deep', 'learning', 'isn', 't', 'an', 'algorithm', 'per', 'se', 'but', 'rather', 'a', 'family', 'of', 'algorithms', 'that', 'implements', 'deep', 'networks', 'many', 'layers', '.', 'These', 'networks', 'are', 'so', 'deep', 'that', 'new', 'methods', 'of', 'computation', 'such', 'as', 'graphics', 'processing', 'units', 'GPUs', 'are', 'required', 'to', 'train', 'them', 'in', 'addition', 'to', 'clusters', 'of', 'compute', 'nodes.', 'So', 'using', 'deep', 'learning', 'we', 'can', 'take', 'detect', 'the', 'employee', 'using', 'face', 'and', 'person', 'recognition', 'scan', 'and', 'through', 'which', 'login', 'logout', 'timing', 'is', 'recorded.', 'Using', 'an', 'AI', 'system', 'we', 'can', 'even', 'identify', 'each', 'employee', 's', 'entry', 'time', 'their', 'working', 'hours', 'non', 'working', 'hours', 'by', 'tracking', 'the', 'movement', 'of', 'an', 'employee', 'in', 'the', 'office', 'so', 'that', 'system', 'can', 'predict', 'and', 'report', 'HR', 'for', 'the', 'salary', 'for', 'each', 'employee', 'based', 'on', 'their', 'working', 'hours.', 'Our', 'system', 'can', 'take', 'feed', 'from', 'CCTV', 'to', 'track', 'movements', 'of', 'employees', 'and', 'this', 'system', 'is', 'capable', 'of', 'recognizing', 'a', 'person', 'even', 'he', 'she', 'is', 'being', 'masked', 'as', 'in', 'this', 'pandemic', 'situation', 'by', 'taking', 'their', 'iris', 'scan.', 'With', 'this', 'system', 'installed', 'inside', 'the', 'office', 'the', 'following', 'are', 'some', 'of', 'the', 'benefits', '1', 'Compliance', 'litigation', 'needs', 'For', 'several', 'countries', 'regulations', 'insist', 'that', 'the', 'employer', 'must', 'keep', 'documents', 'available', 'that', 'can', 'demonstrate', 'the', 'working', 'hours', 'performed', 'by', 'each', 'employee.', 'In', 'the', 'event', 'of', 'control', 'from', 'the', 'labor', 'inspectorate', 'or', 'a', 'dispute', 'with', 'an', 'employee', 'the', 'employer', 'must', 'be', 'able', 'to', 'explain', 'and', 'justify', 'the', 'working', 'hours', 'for', 'the', 'company.', 'This', 'can', 'be', 'made', 'easy', 'as', 'our', 'system', 'is', 'tracking', 'employee', 'movements', '2', 'Information', 'security', 'needs', 'This', 'is', 'about', 'monitoring', 'user', 'connection', 'times', 'to', 'detect', 'suspicious', 'access', 'times.', 'In', 'the', 'event', 'where', 'compromised', 'credentials', 'are', 'used', 'to', 'log', 'on', 'at', '3', 'a.m.', 'on', 'a', 'Saturday', 'a', 'notification', 'on', 'this', 'access', 'could', 'alert', 'the', 'IT', 'team', 'that', 'an', 'attack', 'is', 'possibly', 'underway.', '3', 'Employee', 'login', 'logout', 'software', 'To', 'manage', 'and', 'react', 'to', 'employees', 'attendance', 'overtime', 'thresholds', 'productivity', 'and', 'suspicious', 'access', 'times', 'our', 'system', 'records', 'and', 'stores', 'detailed', 'and', 'interactive', 'reporting', 'on', 'users', 'connection', 'times.', 'These', 'records', 'allow', 'you', 'to', 'better', 'manage', 'users', 'connection', 'times', 'and', 'provide', 'accurate', 'detailed', 'data', 'required', 'by', 'management.', '4', 'If', 'you', 'want', 'to', 'avoid', 'paying', 'overtime', 'make', 'sure', 'that', 'your', 'employees', 'respect', 'certain', 'working', 'time', 'quotas', 'or', 'even', 'avoid', 'suspicious', 'access.', 'Our', 'system', 'will', 'alert', 'the', 'HR', 'officer', 'about', 'each', 'employee', 's', 'office', 'in', 'and', 'out', 'time', 'so', 'that', 'they', 'can', 'accordingly', 'take', 'action.', '5', 'Last', 'but', 'not', 'least', 'it', 'reduces', 'human', 'resource', 'needs', 'to', 'keep', 'track', 'of', 'the', 'records', 'and', 'sending', 'the', 'report', 'to', 'HR', 'and', 'HR', 'officials', 'has', 'to', 'check', 'through', 'the', 'report', 'so', 'this', 'system', 'will', 'reduce', 'times', 'and', 'human', 'resource', 'needs', 'With', 'the', 'use', 'of', 'AI', 'and', 'Deep', 'Learning', 'technologies', 'we', 'can', 'automate', 'some', 'routines', 'stuff', 'with', 'more', 'functionality', 'which', 'humans', 'need', 'more', 'resources', 'to', 'keep', 'track', 'thereby', 'reducing', 'time', 'spent', 'on', 'manual', 'data', 'entry', 'works', 'rather', 'companies', 'can', 'think', 'of', 'making', 'their', 'position', 'high', 'in', 'the', 'competitive', 'world.', 'Blackcoffer', 'Insights', '33', 'Suriya', 'E', 'Vellore', 'Institute', 'of', 'Technology']\n",
      "751\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "text=new_inp['text'][0]\n",
    "#original_text\n",
    "#     print(text)\n",
    "text= regexp_tokenize(text, \"[\\w'\\w.]+\")\n",
    "#count personal words\n",
    "pers_word= 0\n",
    "for w in text:\n",
    "    if w in [\"i\", \"we\", \"my\", \"our\", \"us\", \"I\", \"WE\", \"MY\", \"OUR\", \"US\"]:\n",
    "        pers_word+=1\n",
    "#average word length \n",
    "# count=0\n",
    "L =[len(w) for w in text]\n",
    "print(L)\n",
    "total_char = sum(L)\n",
    "# print(count)          \n",
    "print(text)\n",
    "total_words_para = int(len(text))\n",
    "print(total_words_para)\n",
    "text = \" \".join(text)\n",
    "avg_word_len = round(total_char/total_words_para, 2)\n",
    "# print(avg_word_len)\n",
    "#     print(text)\n",
    "#     nltk.download('punkt')\n",
    "number_of_sentences = len(sent_tokenize(text))\n",
    "print(number_of_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd24ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_in_sent = int(total_words_para)/int(number_of_sentences)\n",
    "print(avg_word_in_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa65b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
